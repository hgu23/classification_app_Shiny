##Prediction
pred1 <- predict(model4, newdata = test)
rmse <- sqrt(sum((exp(pred1) - test$medv)^2)/length(test$medv))
c(RMSE = rmse, R2=summary(model4)$r.squared)
rmse <- sqrt(sum((exp(pred1) - test$medv)^2)/length(test$medv))
c(RMSE = rmse, R2=summary(model4)$r.squared)
exp(2)
grid.arrange(plot1,plot2,plot3,plot4,plot5,plot6,plot7,plot8,plot9,plot10,ncol=5,nrow=2)
library(MASS)
library(ggplot2)
attach(Boston)
names(Boston)
help(Boston)
pairs(Boston)
library(MASS)
library(ggplot2)
attach(Boston)
names(Boston)
help(Boston)
pairs(Boston)
##Sample the dataset. The return for this is row nos.
set.seed(1)
row.number <- sample(1:nrow(Boston), 0.8*nrow(Boston))
train = Boston[row.number,]
test = Boston[-row.number,]
dim(train)
dim(test)
#Let's make default model.
model1 = lm(log(medv)~., data=train)
summary(model1)
par(mfrow=c(2,2))
plot(model1)
# backward search using t-values (alpha level 0.05)
model2 = update(model1, ~.-age)
summary(model2)
model3 = update(model2, ~.-indus)
summary(model3)
par(mfrow=c(2,2))
plot(model3)
train = Boston[row.number,]
test = Boston[-row.number,]
dim(train)
dim(test)
#Let's make default model.
model1 = lm(log(medv)~., data=train)
summary(model1)
par(mfrow=c(2,2))
plot(model1)
summary(model3)
plot(model3)
# Automatic backward search using AIC
help(stepAIC)
?stepAIC
help(stepAIC)
model4 <- stepAIC(model1, direction="backward")
summary(model4)
# Automatic forward search using AIC
model0 = lm(log(medv)~1, data=train)
summary(model0)
model5 <- stepAIC(model0, direction="forward",scope=list(upper=model1,lower=model0))
summary(model5)
# Automatic stepwise search using AIC
model6 <- stepAIC(model0, direction="both",scope=list(upper=model1,lower=model0))
summary(model6)
# Automatic stepwise search using AIC
model6 <- stepAIC(model0, direction="both",scope=list(upper=model1,lower=model0))
summary(model6)
# Automatic exhaustive search
library(leaps)
reg.out=regsubsets(log(medv) ~ ., data = train,nvmax=14)
summary(reg.out)
?regsubsets
pred1 <- predict(model5, newdata = test)
rmse <- sqrt(sum((exp(pred1) - test$medv)^2)/length(test$medv))
c(RMSE = rmse, R2=summary(model5)$r.squared)
install.packages("shinycssloaders")
shiny::runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/Assignment3_app')
install.packages(c("doParallel", "shinyBS"))
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/Assignment3_app')
library(ISLR)
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters=na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
grid=10^seq(10,-2,length=100)  # a range of values to try for lambda
plot(grid)  # notice how they y tends towards zero as x moves forward
library(ISLR)  # datasets from the textbook
library(glmnet)  # generalised linear models
library(dplyr)  # dataset manipulation
library(tidyr)  # more dataset maniupaltion
Hitters = na.omit(Hitters)  # removes rows with "N/A" values
# create matricies for the regression equation
x=model.matrix(Salary~.,Hitters)[,-1]
y = Hitters %>%
select(Salary) %>%
unlist() %>%
as.numeric()
x
View(x)
View(x)
View(y)
?glmnet
plot(ridge_mod)    # Draw plot of coefficients
dim(coef(ridge_mod))  # should be 20 (no. variables in Hitters) x 100 (size of grid)
plot(ridge_mod)
# Ridge Regression and the Lasso
if (0) {
install.packages("ISLR")
install.packages('glmnet',dependencies=TRUE)
install.packages('dplyr')
install.packages('tidyr')
}
library(ISLR)  # datasets from the textbook
library(glmnet)  # generalised linear models
library(dplyr)  # dataset manipulation
library(tidyr)  # more dataset maniupaltion
Hitters = na.omit(Hitters)  # removes rows with "N/A" values
# create matricies for the regression equation
x=model.matrix(Salary~.,Hitters)[,-1]
y = Hitters %>%
select(Salary) %>%
unlist() %>%
as.numeric()
# Ridge Regression
grid=10^seq(10,-2,length=100)  # a range of values to try for lambda
plot(grid)  # notice how they y tends towards zero as x moves forward
ridge_mod = glmnet(x, y, alpha = 0, lambda = grid)  # our ridge regression model
dim(coef(ridge_mod))  # should be 20 (no. variables in Hitters) x 100 (size of grid)
plot(ridge_mod)
ridge_mod
# what we are really intrested in is the l2 norm. typically, coefficient estimates
# (in terms of the l2 norm) are smaller when a large value of lambda is used
ridge_mod$lambda[50] #Display 50th lambda value
ridge_mod$lambda
ridge_mod
# what we are really intrested in is the l2 norm. typically, coefficient estimates
# (in terms of the l2 norm) are smaller when a large value of lambda is used
ridge_mod$lambda[50] #Display 50th lambda value
coef(ridge_mod)[,50] # Display coefficients associated with 50th lambda value
sqrt(sum(coef(ridge_mod)[-1,50]^2)) # Calculate l2 norm
ridge_mod$lambda[60] #Display 60th lambda value
coef(ridge_mod)[,60] # Display coefficients associated with 60th lambda value
sqrt(sum(coef(ridge_mod)[-1,60]^2)) # Calculate l2 norm
sqrt(sum(coef(ridge_mod)[-1,60]^2)) # Calculate l2 norm
# lets go with lambda = 50 for now. this is how we can get the ridge coefficients:
predict(ridge_mod, s = 50, type = "coefficients")[1:20,]
set.seed(1)  # ALWAYS seed your random number generator if you want reproducible results in model development
set.seed(1)
cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
x_train = model.matrix(Salary~., train)[,-1]
x_test = model.matrix(Salary~., test)[,-1]
y_train = train %>%
select(Salary) %>%
unlist() %>%
as.numeric()
y_test = test %>%
select(Salary) %>%
unlist() %>%
as.numeric()
# fit ridge model on training set
ridge_mod = glmnet(x_train, y_train, alpha=0, lambda = grid, thresh = 1e-12)
# predict and work out MSE for test set
ridge_pred = predict(ridge_mod, s = 4, newx = x_test)
mean((ridge_pred - y_test)^2)
# lets calculate MSE as if we had fit the model using only an intercept:
mean((mean(y_train) - y_test)^2)
# this will yield a similar MSE to if we used a very large value of lambda
ridge_pred = predict(ridge_mod, s = 1e10, newx = x_test)
mean((ridge_pred - y_test)^2)
# how does this compare with least squares regression, where lambda = 0?
ridge_pred = predict(ridge_mod, s = 0, newx = x_test)
mean((ridge_pred - y_test)^2)
# as expected, the coefficients are quite similar between a least squares linear model and ridge with lambda = 0
lm(Salary~., data = train)
predict(ridge_mod, s = 0, type="coefficients")[1:20,]
# lets be smart and select the best lambda via cross validation
set.seed(1)
cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
plot(cv.out) # Draw plot of training MSE as a function of lambda
log(bestlam)
set.seed(1)
cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
# Ridge Regression and the Lasso
if (0) {
install.packages("ISLR")
install.packages('glmnet',dependencies=TRUE)
install.packages('dplyr')
install.packages('tidyr')
}
library(ISLR)  # datasets from the textbook
library(glmnet)  # generalised linear models
library(dplyr)  # dataset manipulation
library(tidyr)  # more dataset maniupaltion
Hitters = na.omit(Hitters)  # removes rows with "N/A" values
# create matricies for the regression equation
x=model.matrix(Salary~.,Hitters)[,-1]
y = Hitters %>%
select(Salary) %>%
unlist() %>%
as.numeric()
# Ridge Regression
grid=10^seq(10,-2,length=100)  # a range of values to try for lambda
plot(grid)  # notice how they y tends towards zero as x moves forward
ridge_mod = glmnet(x, y, alpha = 0, lambda = grid)  # our ridge regression model
dim(coef(ridge_mod))  # should be 20 (no. variables in Hitters) x 100 (size of grid)
plot(ridge_mod)    # Draw plot of coefficients
# l1 norm isn't that interesting (is just the sum of magnitudes of coefficients)
# what we are really intrested in is the l2 norm. typically, coefficient estimates
# (in terms of the l2 norm) are smaller when a large value of lambda is used
ridge_mod$lambda[50] #Display 50th lambda value
coef(ridge_mod)[,50] # Display coefficients associated with 50th lambda value
sqrt(sum(coef(ridge_mod)[-1,50]^2)) # Calculate l2 norm
ridge_mod$lambda[60] #Display 60th lambda value
coef(ridge_mod)[,60] # Display coefficients associated with 60th lambda value
sqrt(sum(coef(ridge_mod)[-1,60]^2)) # Calculate l2 norm
# lets go with lambda = 50 for now. this is how we can get the ridge coefficients:
predict(ridge_mod, s = 50, type = "coefficients")[1:20,]
# larger values are more important; ridge can tend towards (but never become) zero
# as variables have less of an impact on the model
# now we split the samples into training and test sets to estimate the error of ridge regression
# we will use the same set when estimating the error of lasso, later
set.seed(1)  # ALWAYS seed your random number generator if you want reproducible results in model development
# the value doesn't really matter. 0 or 1 are common but any reasonable integer will work.
# 50/50 split
train = Hitters %>%
sample_frac(0.5)
test = Hitters %>%
setdiff(train)
x_train = model.matrix(Salary~., train)[,-1]
x_test = model.matrix(Salary~., test)[,-1]
y_train = train %>%
select(Salary) %>%
unlist() %>%
as.numeric()
y_test = test %>%
select(Salary) %>%
unlist() %>%
as.numeric()
# fit ridge model on training set
ridge_mod = glmnet(x_train, y_train, alpha=0, lambda = grid, thresh = 1e-12)
# predict and work out MSE for test set
ridge_pred = predict(ridge_mod, s = 4, newx = x_test)
mean((ridge_pred - y_test)^2)
# lets calculate MSE as if we had fit the model using only an intercept:
mean((mean(y_train) - y_test)^2)
# this will yield a similar MSE to if we used a very large value of lambda
ridge_pred = predict(ridge_mod, s = 1e10, newx = x_test)
mean((ridge_pred - y_test)^2)
# how does this compare with least squares regression, where lambda = 0?
ridge_pred = predict(ridge_mod, s = 0, newx = x_test)
mean((ridge_pred - y_test)^2)
# as expected, the coefficients are quite similar between a least squares linear model and ridge with lambda = 0
lm(Salary~., data = train)
predict(ridge_mod, s = 0, type="coefficients")[1:20,]
# lets be smart and select the best lambda via cross validation
set.seed(1)
cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model on training data
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
plot(cv.out) # Draw plot of training MSE as a function of lambda
log(bestlam)
ridge_pred = predict(ridge_mod, s = bestlam, newx = x_test) # Use best lambda to predict test data
mean((ridge_pred - y_test)^2) # Calculate test MSE
# now that we have our best lambda, we can try generating a model on the entire dataset
out = glmnet(x, y, alpha = 0) # Fit ridge regression model on full dataset
predict(out, type = "coefficients", s = bestlam)[1:20,] # Display coefficients using lambda chosen by CV
# now that we have our best lambda, we can try generating a model on the entire dataset
out = glmnet(x, y, alpha = 0) # Fit ridge regression model on full dataset
predict(out, type = "coefficients", s = bestlam)[1:20,] # Display coefficients using lambda chosen by CV
lasso_mod=glmnet(x_train,y_train,alpha=1,lambda=grid) #fit lasso model on training data
plot(lasso_mod)
set.seed(1)
cv.out=cv.glmnet(x_train,y_train,alpha=1)           #Fit lasso model on training data
plot(cv.out)
bestlam=cv.out$lambda.min                           #Select lambda that minimises training data
lasso_pred=predict(lasso_mod,s=bestlam,newx=x_test) #Use best lambda to predict test data
mean((lasso_pred-y_test)^2) #calculate test MSE
out=glmnet(x,y,alpha=1,lambda=grid) #Fit lasso model on the full dataset
lasso_coeff=predict(out,type="coefficients",s=bestlam)[1:20,] #Display coefficients using lambda chosen by CV
lasso_coeff
# lasso can have some coefficients set to zero.
lasso_coeff[lasso_coeff !=0] # display non-zeros only
lasso_coeff[lasso_coeff == 0]  # display zeros only
install.packages("VIM")
?Knn
?kNN
library(VIM)
?kNN
shiny::runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/Assignment3_app')
runApp()
runApp()
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/Assignment3_app')
install.packages("earth")
install.packages(c("Boruta", "kernlab"))
library(earth)
data(etitanic)
str(etitanic)
head(stats::model.matrix(survived ~ ., data = etitanic))
data(mdrr)
data.frame(table(mdrrDescr$nR11))
nzv <- nearZeroVar(mdrrDescr, saveMetrics= TRUE)
nzv[nzv$nzv,][1:10,]
nzv <- nearZeroVar(mdrrDescr, saveMetrics= TRUE)
nzv[nzv$nzv,][1:10,]
dim(mdrrDescr)
dim(mdrrDescr)
nzv <- nearZeroVar(mdrrDescr)
filteredDescr <- mdrrDescr[, -nzv]
dim(filteredDescr)
set.seed(96)
## 50% randomised split (unstratified)
inTrain <- sample(seq(along = mdrrClass), length(mdrrClass)/2)
training <- filteredDescr[inTrain,]
test <- filteredDescr[-inTrain,]
#train the preprocessing
preProcValues <- preProcess(training, method = c("center", "scale"))
preProcValues
#Apply to both train and test
trainTransformed <- predict(preProcValues, training)
testTransformed <- predict(preProcValues, test)
head(trainTransformed)
#train the preprocessing
preProcValues <- preProcess(training, method = c("center", "scale", "knnImpute"))  ### This is the only bit that has changed
preProcValues
#Apply to both train and test
trainTransformed <- predict(preProcValues, training)
testTransformed <- predict(preProcValues, test)
head(trainTransformed)
#train the preprocessing
preProcValues <- preProcess(training, method = c("center", "scale", "knnImpute", "pca"))  ### This is the only bit that has changed
preProcValues
#Apply to both train and test
trainTransformed <- predict(preProcValues, training)
testTransformed <- predict(preProcValues, test)
head(trainTransformed)
?train
ifelse()
?ifelse
?train
?train
#train the preprocessing
preProcValues <- preProcess(training, method = c("YeoJohnson", "center", "scale", "knnImpute", "pca"))
#Apply to both train and test
trainTransformed <- predict(preProcValues, training)
testTransformed <- predict(preProcValues, test)
data <- cbind(mdrrClass[inTrain], trainTransformed)
colnames(data)[1] <- "mdrrClass"
# Train a CART model using the preprocessed data
CARTModel <- caret::train(mdrrClass ~ .,
data = data,
method = "rpart",
trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
tuneLength = 5
)
CARTModel
set.seed(96)
## 50% randomised split (unstratified)
inTrain <- sample(seq(along = mdrrClass), length(mdrrClass)/2)
training <- filteredDescr[inTrain,]
test <- filteredDescr[-inTrain,]
data <- cbind(mdrrClass[inTrain], training)
colnames(data)[1] <- "mdrrClass"
# Train a CART model using the preprocessed data
CARTModel <- caret::train(mdrrClass ~ .,
data = data,
method = "rpart",
preProcess = c("YeoJohnson", "center", "scale", "knnImpute", "pca"),   # <- this is the magic
trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
tuneLength = 5
)
CARTModel
shiny::runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/Assignment3_app')
shiny::runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp()
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
?function
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
shiny::runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
?if
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
shiny::runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
?tryCatch
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
runApp('C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app')
data = read.csv("test.csv")
setwd("C:/Desktop/Lecture Notes MADS/DSI/Assignments/classification_app")
data = read.csv("test.csv")
data[1:5,]
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
data =  read.csv("test.csv")
vis_dat(data)
train = read.csv('train.csv')
vis_dat(train)
View(data)
runApp()
runApp()
runApp()
vis_dat(train)
df = train
df$Pclass = as.factor(df$Pclass)
df$SibSp = as.factor(df$SibSp)
df$Parch = factor(df$Parch, levels = c("0","1","2","3","4","5","6","9"))
df$Name = NULL
df$Ticket = NULL
df$Cabin = NULL
df
test = data
df = getData2()
df$Pclass = as.factor(df$Pclass)
df$SibSp = as.factor(df$SibSp)
df$Parch = factor(df$Parch, levels = c("0","1","2","3","4","5","6","9"))
df$Name = NULL
df$Ticket = NULL
df$Cabin = NULL
df
df = test
df$Pclass = as.factor(df$Pclass)
df$SibSp = as.factor(df$SibSp)
df$Parch = factor(df$Parch, levels = c("0","1","2","3","4","5","6","9"))
df$Name = NULL
df$Ticket = NULL
df$Cabin = NULL
df
vis_dat(test)
df2 = test
df$Pclass = as.factor(df$Pclass)
df$SibSp = as.factor(df$SibSp)
df$Parch = factor(df$Parch, levels = c("0","1","2","3","4","5","6","9"))
df$Name = NULL
df$Ticket = NULL
df$Cabin = NULL
df
df = train
df$Survived = as.factor(df$Survived)
df$Pclass = as.factor(df$Pclass)
df$SibSp = as.factor(df$SibSp)
df$Parch = as.factor(df$Parch)
df$Name = NULL
df$Ticket = NULL
df$Cabin = NULL
df
vis_dat(df2)
df2 = test
df2$Pclass = as.factor(df2$Pclass)
df2$SibSp = as.factor(df2$SibSp)
df2$Parch = factor(df2$Parch, levels = c("0","1","2","3","4","5","6","9"))
df2$Name = NULL
df2$Ticket = NULL
df2$Cabin = NULL
df2
vis_dat(df)
vis_dat(df2)
nlevels(df2$Pclass)
nlevels(df$Pclass)
nlevels(df2$Pclass)
nlevels(df2$Sex)
nlevels(df$Sex)
nlevels(df2$SibSp)
nlevels(df$SibSp)
nlevels(df2$Parch)
nlevels(df$Parch)
nlevels(df2$Embarked)
nlevels(df$Embarked)
num_levels_train = df %>%
sapply(nlevels)
num_levels_test = df2 %>%
sapply(nlevels)
num_levels_test
num_levels_train
distinct(df$Embarked)
unique(df$Embarked)
unique(df2$Embarked)
View(df2)
runApp()
runApp()
runApp()
runApp()
